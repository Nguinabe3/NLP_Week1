{"cells":[{"cell_type":"markdown","metadata":{"id":"thobkiOZ76P9"},"source":["\n","<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI_2024_NLP - Week 1</h1>\n","\n","<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 3: N-gram models </h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AO2Q6hrQ76QC"},"outputs":[],"source":["import io, sys, math, re\n","from collections import defaultdict\n","import numpy as np"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"wG9pqSoV76QD"},"outputs":[],"source":["# data_loader\n","def load_data(filename):\n","    '''\n","    parameters:\n","    filename (string): datafile\n","\n","    Returns:\n","    data (list of lists): each list is a sentence of the text\n","    vocab (dictionary): {word: no of times it appears in the text}\n","    '''\n","    fin = io.open(filename, 'r', encoding='utf-8')\n","    data = []\n","    vocab = defaultdict(lambda:0)\n","    for line in fin:\n","        sentence = line.split()\n","        data.append(sentence)\n","        for word in sentence:\n","            vocab[word] += 1\n","    return data, vocab"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"amQ02wsD76QE","outputId":"fb7e6f00-fad7-4240-f6ec-4c247519389f"},"outputs":[{"name":"stdout","output_type":"stream","text":["load training set..\n","\n","\n","['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n","\n","\n","how : 107\n","load validation set\n"]}],"source":["\n","print(\"load training set..\")\n","print(\"\\n\")\n","train_data, vocab = load_data(\"train1.txt\")\n","print(train_data[0])\n","print(\"\\n\")\n","print(\"how :\",vocab['how'])\n","print(\"load validation set\")\n","valid_data, _ = load_data(\"valid1.txt\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"6kfh1SAS76QE"},"outputs":[],"source":["def remove_rare_words(data, vocab, mincount=1):\n","    '''\n","    Parameters:\n","    data (list of lists): each list is a sentence of the text\n","    vocab (dictionary): {word: no of times it appears in the text}\n","    mincount(int): the minimum count\n","\n","    Returns:\n","    data_with_unk(list of lists): data after replacing rare words with <unk> token\n","    '''\n","    data_with_unk = []\n","    ##########################################################################\n","    #                      TODO: Implement this function                     #\n","    ##########################################################################\n","    for sentence in data:\n","        new_sentence = []\n","        for word in sentence:\n","            if word in vocab and vocab[word] >= mincount:\n","                new_sentence.append(word)\n","            else:\n","                new_sentence.append('<unk>')\n","        data_with_unk.append(new_sentence)\n","\n","    return data_with_unk\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEB_VcVj76QF","outputId":"48943fb9-d0d3-4593-887d-e1235fbc9768"},"outputs":[{"name":"stdout","output_type":"stream","text":["remove rare words\n","['<s>', 'my', '<unk>', \"don't\", 'speak', '<unk>', '</s>']\n"]}],"source":["print(\"remove rare words\")\n","\n","train_data = remove_rare_words(train_data, vocab, mincount = 2)\n","valid_data = remove_rare_words(valid_data, vocab, mincount = 1)\n","#train_data\n","print(train_data[0])"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","def build_ngram(data, n):\n","    '''\n","    Parameters:\n","    data (list of lists): each list is a sentence of the text\n","    n (int): size of the n-gram\n","\n","    Returns:\n","    prob (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    '''\n","    total_number_words = 0\n","    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","    def _build_ngram(data, n):\n","        counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","        for sentence in data:\n","            sentence = tuple(sentence)\n","            ##########################################################################\n","          #                      TODO: Implement this function                     #\n","          # dict can be indexed by tuples\n","          # store in the same dict all the ngrams\n","          # by using the context as a key and the word as a value\n","          ##########################################################################\n","            for i in range(len(sentence) - n + 1):\n","                context = sentence[i:i+n-1]\n","                word = sentence[i+n-1]\n","                counts[context][word] += 1\n","          ##########################################################################\n","          #                            END OF YOUR CODE                            #\n","          ##########################################################################\n","        prob = defaultdict(lambda: defaultdict(lambda: 0.0))\n","        # Build the probabilities from the counts\n","        # Be careful with how you normalize!\n","\n","        for context in counts.keys():\n","            # p(w | context) = count(context, w)/ count(context)\n","            ##########################################################################\n","            #                      TODO: Implement this function                     #\n","            ##########################################################################\n","\n","            total_count = sum(counts[context].values())\n","            for word in counts[context].keys():\n","                prob[context][word] = counts[context][word] / total_count\n","            ##########################################################################\n","            #                            END OF YOUR CODE                            #\n","            ##########################################################################\n","        return prob\n","\n","    prob = defaultdict(lambda: defaultdict(lambda: 0.0))  # Initialization\n","    for i in range(1, n + 1):\n","        ngram_prob = _build_ngram(data, i)\n","        for context in ngram_prob.keys():\n","            for word in ngram_prob[context].keys():\n","                prob[context][word] = ngram_prob[context][word]\n","\n","    return prob"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMxK-Qki76QG","outputId":"ff2e4013-8738-469b-9313-3c875d73b925"},"outputs":[{"name":"stdout","output_type":"stream","text":["build ngram model with n =  10\n"]}],"source":["# RUN TO BUILD NGRAM MODEL\n","n = 10\n","print(\"build ngram model with n = \", n)\n","model = build_ngram(train_data, n)\n"]},{"cell_type":"markdown","metadata":{"id":"KbCzRXJk76QG"},"source":["Here, implement a recursive function over shorter and shorter context to compute a \"stupid backoff model\". An interpolation model can also be implemented this way."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def get_prob(model, context, w):\n","    '''\n","    Parameters:\n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    context (list of strings): a sentence\n","    w(string): the word we need to find its probability given the context\n","    discount (float): discount factor for backoff\n","\n","    Returns:\n","    prob (float): probability of this word given the context\n","    '''\n","    # code a recursive function over\n","    # smaller and smaller context\n","    # to compute the backoff model\n","    ##########################################################################\n","    #                      TODO: Implement this function                     #\n","    ##########################################################################\n","\n","    # Base case: if context is empty, return the unigram probability\n","    if len(context) == 0:\n","        return model.get((), {}).get(w, 0)\n","\n","    # Convert context to tuple for consistent dictionary key access\n","    context_tuple = tuple(context)\n","\n","    # Check if the word is in the given context\n","    if context_tuple in model and w in model[context_tuple]:\n","        return model[context_tuple][w]\n","    else:\n","        # Recursively call get_prob with a smaller context (backoff)\n","        shorter_context_prob = get_prob(model, context[1:], w)\n","        return shorter_context_prob\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"z5waVP3C76QH"},"outputs":[],"source":["def perplexity(model, data, n):\n","    '''\n","    Parameters:\n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    data (list of lists): each list is a sentence of the text\n","    n(int): size of the n-gram\n","\n","    Retunrs:\n","    perp(float): the perplexity of the model\n","    '''\n","    ##########################################################################\n","    #                      TODO: Implement this function                     #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","    l_p_s = 0.0\n","    t_w = 0\n","\n","    for sentence in data:\n","        sentence = ['<s>'] * (n - 1) + sentence + ['</s>']  # Add start and end tokens\n","        for i in range(n - 1, len(sentence)):\n","            context = sentence[i - n + 1:i]\n","            word = sentence[i]\n","            prob = get_prob(model, context, word)\n","            if prob > 0:\n","                l_p_s += math.log(prob, 2)\n","            else:\n","                l_p_s += float('-inf')\n","            t_w += 1\n","\n","    perplexity = math.pow(2, -l_p_s / t_w)\n","    return perplexity\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYBc5Aam76QH","outputId":"ee54fd0a-fe39-4e25-b479-a2d62274a623"},"outputs":[{"name":"stdout","output_type":"stream","text":["The perplexity is inf\n"]}],"source":["# COMPUTE PERPLEXITY ON VALIDATION SET\n","\n","print(\"The perplexity is\", perplexity(model, valid_data, n=n))"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["def get_proba_distrib(model, context):\n","    '''\n","    Parameters:\n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    context (list of strings): the sentence we need to find the words after it and\n","    their probabilities\n","\n","    Returns:\n","    words_and_probs(dic): {word: probability of word given context}\n","    '''\n","    # code a recursive function over context\n","    # to find the longest available ngram\n","    ##########################################################################\n","    #                      TODO: Implement this function                     #\n","    ##########################################################################\n","    def _get_proba_distrib(context):\n","        context_tuple = tuple(context)\n","\n","        if context_tuple in model:\n","            return model[context_tuple]\n","        elif len(context) == 0:\n","            return {}\n","        else:\n","            return _get_proba_distrib(context[1:])\n","\n","    return _get_proba_distrib(context)\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def generate(model):\n","    '''\n","    Parameters:\n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","\n","    Returns:\n","    sentence (list of strings): a sentence sampled according to the language model.\n","    '''\n","    # generate a sentence. A sentence starts with a <s> and ends with a </s>\n","    # Possiblly a use function is:\n","    # np.random.choice(x, 1, p = y)\n","\n","    # where x is a list of things to sample from\n","    # and y is a list of probability (of the same length as x)\n","    \n","    #print (model[(\"<s>\")])\n","    #print (len(model[tuple(sentence)].values()))\n","    sentence = [\"<s>\"]\n","\n","    while sentence[-1] != \"</s>\" and len(sentence) < 10:\n","        ##########################################################################\n","        #                      TODO: Implement this function                     #\n","        ##########################################################################\n","        context = sentence[-(len(model) - 1):]\n","        proba_distrib = get_proba_distrib(model, context)\n","        \n","        if not proba_distrib:\n","            break\n","\n","        words = list(proba_distrib.keys())\n","        probs = list(proba_distrib.values())\n","\n","        next_word = np.random.choice(words, 1, p=probs)[0]\n","        sentence.append(next_word)\n","        ##########################################################################\n","        #                            END OF YOUR CODE                            #\n","        ##########################################################################\n","    return sentence\n","    "]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWqzUXjw76QI","outputId":"ef1984bc-acf7-4f85-9c40-1e0d9c214839"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated sentence:  ['<s>', 'he', 'left', 'the', 'money', 'at', 'home.', '</s>']\n"]}],"source":["# GENERATE A SENTENCE FROM THE MODEL\n","\n","print(\"Generated sentence: \",generate(model))"]},{"cell_type":"markdown","metadata":{"id":"XCs2pG6P76QJ"},"source":["Once you are done implementing the model, evaluation and generation code, you can try changing the value of `n`, and play with a larger training set (`train2.txt` and `valid2.txt`). You can also try to implement an interpolation model."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INh4pNmm76QJ","outputId":"19fe5739-20dc-4285-a227-76f8212020b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["load training set 2..\n","\n","\n","['<s>', 'i', 'liked', 'your', 'idea', 'and', 'adopted', 'it', '.', '</s>']\n","\n","\n","how : 3195\n","load validation set 2\n"]}],"source":["print(\"load training set 2..\")\n","print(\"\\n\")\n","train_data2, vocab = load_data(\"train2.txt\")\n","print(train_data2[0])\n","print(\"\\n\")\n","print(\"how :\",vocab['how'])\n","print(\"load validation set 2\")\n","valid_data2, _ = load_data(\"valid2.txt\")\n"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"xyEO8rKZB6SR"},"outputs":[],"source":["n = 3"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"tq71nPJjBZRR"},"outputs":[],"source":["model = build_ngram(train_data2, n)"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHcKA9nxBpXk","outputId":"ebd44ab9-f17a-406e-affb-0db0df5766d7"},"outputs":[{"data":{"text/plain":["23.940639090840747"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["perplexity(model,valid_data2, n)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7vh3LzcBvHj","outputId":"6be8fd9f-711e-484b-b7ec-af11e6bb5769"},"outputs":[{"data":{"text/plain":["['<s>', 'i', 'cast', 'my', 'net', 'into', 'the', 'navy', '<unk>', 'our']"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["generate(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f-xb6wBB3tn"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
