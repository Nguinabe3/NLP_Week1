{"cells":[{"cell_type":"markdown","metadata":{"id":"AW1aaO3bvcbJ"},"source":["# MMI_2024_NLP - Week 1\n","\n","# Lab 1: Part 1"]},{"cell_type":"markdown","metadata":{"id":"5kQJhsHhn4hb"},"source":["# (A) Naive Bayes model"]},{"cell_type":"markdown","metadata":{"id":"aRqye55SXDsa"},"source":["In this lab, we will implement a language identifier (LID).\n","\n","Our first model will be based on Naive Bayes."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jlo96veyXDsb"},"outputs":[],"source":["import io, sys, math, re\n","from collections import defaultdict\n","from typing import List, Tuple, Dict"]},{"cell_type":"markdown","metadata":{"id":"9UdfI-OmXDsc"},"source":["The next function is used to load the data. Each line of the data consist of a label (corresponding to a language), followed by some text, written in that language. Here is an example of data:\n","\n","```__label__de Zur Namensdeutung gibt es mehrere Varianten.```\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CjHGIo9gXDsc"},"outputs":[],"source":["def load_data(filename:str)->List[Tuple]:\n","    fin = io.open(filename, 'r', encoding='utf-8')\n","    data = []\n","    for line in fin:\n","        tokens = line.split()\n","        data.append((tokens[0], tokens[1:]))\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"cY1bqzizXDsd"},"source":["You can now try loading the first dataset `train1.txt` and look what examples look like."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtlY6M6tXDsd","outputId":"222f88b5-1944-43cc-9c3a-3d5cb1a57141"},"outputs":[{"name":"stdout","output_type":"stream","text":["('__label__de', ['Tom', 'ist', 'an', 'Kunst', 'vÃ¶llig', 'uninteressiert.'])\n"]}],"source":["data = load_data(\"train1.txt\")\n","print(data[1])"]},{"cell_type":"markdown","metadata":{"id":"BuXwK6eRXDsd"},"source":["Next, we will start implementing the Naive Bayes method. This technique is based on word counts, and we thus need to start by implementing a function to count the words and labels of our training set.\n","\n","`n_examples` is the total number of examples\n","\n","`n_words_per_label` is the total number of words for a given label\n","\n","`label_counts` is the number of times a given label appears in the training data\n","\n","`word_counts` is the number of times a word appears with a given label"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"_O6QtghUXDsd"},"outputs":[],"source":["def count_words(data:str)->Dict:\n","    n_examples = 0\n","    n_words_per_label = defaultdict(lambda: 0)\n","    label_counts = defaultdict(lambda: 0)\n","    word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","    for example in data:\n","        label, sentence = example\n","        ##########################################################################\n","        #                      TODO: Implement this function                     #\n","        ##########################################################################\n","        # Replace \"pass\" statement with your code\n","        n_examples += 1\n","        label_counts[label] += 1\n","        #print(sentence)\n","        words = sentence\n","        n_words_per_label[label] += len(words)\n","\n","        for word in words:\n","            word_counts[label][word] += 1\n","        ##########################################################################\n","        #                            END OF YOUR CODE                            #\n","        ##########################################################################\n","\n","        \n","\n","    return {'label_counts': label_counts,\n","            'word_counts': word_counts,\n","            'n_examples': n_examples,\n","            'n_words_per_label': n_words_per_label}"]},{"cell_type":"markdown","metadata":{"id":"u5BBqz2JXDse"},"source":["Next, using the word and label counts from the previous function, we can implement the prediction function.\n","\n","Here, `mu` is a regularization parameter (Laplace smoothing), and `sentence` is the list of words corresponding to the test example."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"46_dbxuDXDse"},"outputs":[],"source":["import numpy as np\n","from typing import Dict, List, Tuple\n","def predict(sentence:List, mu:float, label_counts:Dict, word_counts:Dict, n_examples:int, n_words_per_label:Dict)->str:\n","    best_label = None\n","    best_score = float('-inf')\n","\n","    for label in word_counts.keys():\n","        score = 0.0\n","        prior = label_counts[label] / sum(label_counts.values())\n","        #P(Class | Word) = P(Class) * P(word | Class)\n","        ##########################################################################\n","        #                      TODO: Implement this function                     #\n","        ##########################################################################\n","        # Replace \"pass\" statement with your code\n","        score += np.log(prior)  # Use log to avoid underflow\n","\n","        for word in sentence:\n","            word_likelihood = (word_counts[label][word] + mu) / (n_words_per_label[label] + mu * len(word_counts[label]))\n","            score += np.log(word_likelihood)\n","\n","        if score > best_score:\n","            best_score = score\n","            best_label = label\n","        ##########################################################################\n","        #                            END OF YOUR CODE                            #\n","        ##########################################################################\n","\n","    return best_label"]},{"cell_type":"markdown","metadata":{"id":"uGMlrhFmXDsf"},"source":["The next function will be used to evaluate the Naive Bayes model on a validation set. It computes the accuracy for a particular regularization parameter `mu`."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"QE0EVY6OXDsf"},"outputs":[],"source":["def compute_accuracy(valid_data:str, mu:float, counts:Dict)->float:\n","    accuracy = 0.0\n","    correct_predictions = 0\n","    total_predictions = len(valid_data)\n","    for label, sentence in valid_data:\n","      ##########################################################################\n","      #                      TODO: Implement this function                     #\n","      ##########################################################################\n","      # Replace \"pass\" statement with your code\n","      sentence_words = sentence#.split()\n","\n","      predicted_label = predict(sentence_words, mu, counts['label_counts'], counts['word_counts'], counts['n_examples'], counts['n_words_per_label'])\n","      if predicted_label == label:\n","        correct_predictions += 1\n","\n","      accuracy = correct_predictions / total_predictions\n","      ##########################################################################\n","      #                            END OF YOUR CODE                            #\n","      ##########################################################################\n","\n","    return accuracy # Replace \"...\" statement with your code"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0NY2-oSXDsf","outputId":"6738ff04-224c-4a9d-b07a-3634cf8c0332"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","** Naive Bayes **\n","\n","Validation accuracy: 0.953\n","\n"]}],"source":["print(\"\")\n","print(\"** Naive Bayes **\")\n","print(\"\")\n","\n","mu = 0.2\n","train_data = load_data(\"train1.txt\")\n","valid_data = load_data(\"valid1.txt\")\n","counts = count_words(train_data)\n","\n","print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n","print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"1pYhDzXRuMIw"},"source":["# Now, it is your turn, try to do it with train2.txt and valid2.txt.\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","** Naive Bayes **\n","\n","Validation accuracy: 0.980\n","\n"]}],"source":["import io\n","import numpy as np\n","from collections import defaultdict\n","from typing import List, Tuple, Dict\n","\n","class NaiveBaye_LID:\n","    \n","    def load_data(self, filename: str) -> List[Tuple]:\n","        fin = io.open(filename, 'r', encoding='utf-8')\n","        data = []\n","        for line in fin:\n","            tokens = line.split()\n","            data.append((tokens[0], tokens[1:]))\n","        return data\n","    \n","    def count_words(self, data: List[Tuple]) -> Dict:\n","        n_examples = 0\n","        n_words_per_label = defaultdict(lambda: 0)\n","        label_counts = defaultdict(lambda: 0)\n","        word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","        for example in data:\n","            label, sentence = example\n","            n_examples += 1\n","            label_counts[label] += 1\n","            words = sentence\n","            n_words_per_label[label] += len(words)\n","\n","            for word in words:\n","                word_counts[label][word] += 1\n","\n","        return {'label_counts': label_counts,\n","                'word_counts': word_counts,\n","                'n_examples': n_examples,\n","                'n_words_per_label': n_words_per_label}\n","    \n","    def predict(self, sentence: List[str], mu: float, label_counts: Dict, word_counts: Dict, n_examples: int, n_words_per_label: Dict) -> str:\n","        best_label = None\n","        best_score = float('-inf')\n","\n","        for label in word_counts.keys():\n","            score = 0.0\n","            prior = label_counts[label] / sum(label_counts.values())\n","            score += np.log(prior)  # Use log to avoid underflow\n","\n","            for word in sentence:\n","                word_likelihood = (word_counts[label][word] + mu) / (n_words_per_label[label] + mu * len(word_counts[label]))\n","                score += np.log(word_likelihood)\n","\n","            if score > best_score:\n","                best_score = score\n","                best_label = label\n","\n","        return best_label\n","    \n","    def compute_accuracy(self, valid_data: List[Tuple], mu: float, counts: Dict) -> float:\n","        correct_predictions = 0\n","        total_predictions = len(valid_data)\n","\n","        for label, sentence in valid_data:\n","            sentence_words = sentence\n","            predicted_label = self.predict(sentence_words, mu, counts['label_counts'], counts['word_counts'], counts['n_examples'], counts['n_words_per_label'])\n","            if predicted_label == label:\n","                correct_predictions += 1\n","\n","        accuracy = correct_predictions / total_predictions\n","        return accuracy\n","        \n","    def train(self):\n","        print(\"\")\n","        print(\"** Naive Bayes **\")\n","        print(\"\")\n","\n","        mu = 1.0\n","        train_data = self.load_data(\"train2.txt\")\n","        valid_data = self.load_data(\"valid2.txt\")\n","        counts = self.count_words(train_data)\n","\n","        print(\"Validation accuracy: %.3f\" % self.compute_accuracy(valid_data, mu, counts))\n","        print(\"\")\n","\n","if __name__ == \"__main__\":\n","    NaiveBaye = NaiveBaye_LID()\n","    NaiveBaye.train()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.undefined"}},"nbformat":4,"nbformat_minor":0}
